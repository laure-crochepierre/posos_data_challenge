{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd \n",
    "from utils import train_test_validation_split\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Embedding, Flatten, LSTM, GRU, Concatenate\n",
    "from keras.losses import sparse_categorical_crossentropy, categorical_hinge\n",
    "from keras import optimizers\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping, LearningRateScheduler, History\n",
    "\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "X = pd.read_csv('DATA/clean_data/cleaned_n_stemming_input_train.csv', sep=\";\", index_col=0)\n",
    "y = pd.read_csv('DATA/output_train.csv', sep=\";\", index_col=0)\n",
    "\n",
    "features = X.columns\n",
    "targets = y['intention'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    bonjour tromp forum question alor repos ici pr...\n",
       "1                  <MEDICAMENT> soulag contr <MALADIE>\n",
       "2    medecin <MEDICAMENT> prescr <MEDICAMENT> <ORDI...\n",
       "3       est exist form adapt enfant <AGE> <MEDICAMENT>\n",
       "4    medecin soign <MEDICAMENT> pharyngit <MEDICAME...\n",
       "Name: question, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X['question'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for nn and find correct parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimation of the size of the vocabulary \n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit_transform(X['question'])\n",
    "MAX_NB_WORDS = len(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "382"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find max size of the sequences \n",
    "MAX_SEQUENCE_LENGTH = 0 \n",
    "for sentence in X['question']:\n",
    "    if MAX_SEQUENCE_LENGTH<len(sentence.split()):\n",
    "        MAX_SEQUENCE_LENGTH = len(sentence.split())\n",
    "MAX_SEQUENCE_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4746 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# Preprocess text fo feed the net \n",
    "texts = X['question']\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS/2)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "X_sequences = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, X_validation, y_train, y_test, y_validation = train_test_validation_split(X_sequences,y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0, ...,   0, 545, 744],\n",
       "       [  0,   0,   0, ..., 108, 244,  15],\n",
       "       [  0,   0,   0, ..., 102, 156,  40],\n",
       "       ...,\n",
       "       [  0,   0,   0, ..., 232,  72, 294],\n",
       "       [  0,   0,   0, ..., 110,  26,   1],\n",
       "       [  0,   0,   0, ...,   2,  12,   4]], dtype=int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_unique_logpath(logdir, raw_run_name):\n",
    "        i = 0\n",
    "        while(True):\n",
    "                run_name = raw_run_name + \"-\" + str(i)\n",
    "                log_path = os.path.join(logdir, run_name)\n",
    "                if not os.path.isdir(log_path):\n",
    "                        return log_path\n",
    "                i = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.006\n",
    "    drop = 0.5\n",
    "    epochs_drop = 2.0\n",
    "    lrate = initial_lrate * math.pow(drop,  \n",
    "           math.floor((1+epoch)/epochs_drop))\n",
    "    return lrate\n",
    "\n",
    "def exp_decay(epoch):\n",
    "    initial_lrate = 0.06\n",
    "    k = 0.1\n",
    "    lrate = initial_lrate * np.exp(-k*t)\n",
    "    return lrate\n",
    "\n",
    "lrate = LearningRateScheduler(step_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define parameters \n",
    "EMBEDDING_DIM = 200\n",
    "NB_CATEGORIES = len(targets)\n",
    "NB_LSTM = 50\n",
    "PERC_DROPOUT = 0.8\n",
    "EPOCHS = 30\n",
    "#Define RMSProp optimizer\n",
    "LEARNING_RATE = 0.006\n",
    "RATE_DECAY = LEARNING_RATE / EPOCHS\n",
    "optz = optimizers.RMSprop(lr=LEARNING_RATE, decay=RATE_DECAY)\n",
    "\n",
    "\n",
    "sgd = optimizers.SGD(lr=LEARNING_RATE, momentum=0.9, nesterov=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = \"gru_run_stemming\"\n",
    "\n",
    "logpath = generate_unique_logpath(\"./logs_tensorboard\", run_name)\n",
    "tbcb = TensorBoard(log_dir=logpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 382, 200)          949400    \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 50)                37650     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 51)                2601      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 51)                0         \n",
      "=================================================================\n",
      "Total params: 989,651\n",
      "Trainable params: 989,651\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 5137 samples, validate on 1285 samples\n",
      "Epoch 1/30\n",
      "5137/5137 [==============================] - 39s 7ms/step - loss: 3.0038 - acc: 0.2743 - val_loss: 2.3654 - val_acc: 0.4615\n",
      "Epoch 2/30\n",
      "5137/5137 [==============================] - 39s 8ms/step - loss: 2.3116 - acc: 0.4283 - val_loss: 1.9259 - val_acc: 0.5167\n",
      "Epoch 3/30\n",
      "5137/5137 [==============================] - 41s 8ms/step - loss: 1.9922 - acc: 0.4960 - val_loss: 1.8202 - val_acc: 0.5455\n",
      "Epoch 4/30\n",
      "5137/5137 [==============================] - 40s 8ms/step - loss: 1.8099 - acc: 0.5425 - val_loss: 1.6996 - val_acc: 0.5681\n",
      "Epoch 5/30\n",
      "5137/5137 [==============================] - 40s 8ms/step - loss: 1.6411 - acc: 0.5803 - val_loss: 1.6767 - val_acc: 0.5899\n",
      "Epoch 6/30\n",
      "5137/5137 [==============================] - 41s 8ms/step - loss: 1.5424 - acc: 0.6042 - val_loss: 1.6716 - val_acc: 0.5899\n",
      "Epoch 7/30\n",
      "5137/5137 [==============================] - 39s 8ms/step - loss: 1.4682 - acc: 0.6231 - val_loss: 1.6359 - val_acc: 0.5984\n",
      "Epoch 8/30\n",
      "5137/5137 [==============================] - 39s 8ms/step - loss: 1.3836 - acc: 0.6482 - val_loss: 1.6988 - val_acc: 0.6031\n",
      "Epoch 9/30\n",
      "5137/5137 [==============================] - 39s 8ms/step - loss: 1.3276 - acc: 0.6599 - val_loss: 1.6778 - val_acc: 0.5969\n",
      "Epoch 10/30\n",
      "5137/5137 [==============================] - 39s 8ms/step - loss: 1.2626 - acc: 0.6825 - val_loss: 1.6469 - val_acc: 0.5969\n",
      "Epoch 11/30\n",
      "5137/5137 [==============================] - 39s 8ms/step - loss: 1.2116 - acc: 0.6961 - val_loss: 1.6712 - val_acc: 0.6078\n",
      "Epoch 12/30\n",
      "5137/5137 [==============================] - 39s 8ms/step - loss: 1.1688 - acc: 0.7016 - val_loss: 1.7161 - val_acc: 0.5899\n",
      "Epoch 13/30\n",
      "5137/5137 [==============================] - 39s 8ms/step - loss: 1.1377 - acc: 0.7078 - val_loss: 1.6983 - val_acc: 0.6047\n",
      "Epoch 14/30\n",
      "5137/5137 [==============================] - 39s 8ms/step - loss: 1.1028 - acc: 0.7197 - val_loss: 1.6966 - val_acc: 0.6171\n",
      "Epoch 15/30\n",
      "5137/5137 [==============================] - 40s 8ms/step - loss: 1.0586 - acc: 0.7319 - val_loss: 1.7351 - val_acc: 0.6156\n",
      "Epoch 16/30\n",
      "5137/5137 [==============================] - 40s 8ms/step - loss: 1.0523 - acc: 0.7413 - val_loss: 1.7490 - val_acc: 0.6101\n",
      "Epoch 17/30\n",
      "5137/5137 [==============================] - 50s 10ms/step - loss: 0.9952 - acc: 0.7475 - val_loss: 1.7417 - val_acc: 0.6062\n",
      "Epoch 18/30\n",
      "5137/5137 [==============================] - 44s 9ms/step - loss: 1.0103 - acc: 0.7491 - val_loss: 1.7504 - val_acc: 0.6047\n",
      "Epoch 19/30\n",
      "5137/5137 [==============================] - 43s 8ms/step - loss: 0.9608 - acc: 0.7553 - val_loss: 1.7849 - val_acc: 0.6031\n",
      "Epoch 20/30\n",
      "5137/5137 [==============================] - 41s 8ms/step - loss: 0.9259 - acc: 0.7646 - val_loss: 1.8228 - val_acc: 0.6062\n",
      "Epoch 21/30\n",
      "5137/5137 [==============================] - 39s 8ms/step - loss: 0.8778 - acc: 0.7818 - val_loss: 1.8169 - val_acc: 0.6031\n",
      "Epoch 22/30\n",
      "5137/5137 [==============================] - 39s 8ms/step - loss: 0.8980 - acc: 0.7746 - val_loss: 1.8417 - val_acc: 0.6039\n",
      "Epoch 23/30\n",
      "5137/5137 [==============================] - 39s 8ms/step - loss: 0.8697 - acc: 0.7777 - val_loss: 1.8454 - val_acc: 0.6062\n",
      "Epoch 24/30\n",
      "5137/5137 [==============================] - 39s 8ms/step - loss: 0.8673 - acc: 0.7750 - val_loss: 1.8781 - val_acc: 0.6070\n",
      "Epoch 25/30\n",
      "5137/5137 [==============================] - 39s 8ms/step - loss: 0.8323 - acc: 0.7894 - val_loss: 1.8934 - val_acc: 0.6023\n",
      "Epoch 26/30\n",
      "5137/5137 [==============================] - 39s 8ms/step - loss: 0.8130 - acc: 0.7863 - val_loss: 1.8777 - val_acc: 0.6109\n",
      "Epoch 27/30\n",
      "5137/5137 [==============================] - 39s 8ms/step - loss: 0.8115 - acc: 0.7925 - val_loss: 1.8718 - val_acc: 0.6101\n",
      "Epoch 28/30\n",
      "5137/5137 [==============================] - 39s 8ms/step - loss: 0.7828 - acc: 0.8020 - val_loss: 1.9235 - val_acc: 0.6117\n",
      "Epoch 29/30\n",
      "5137/5137 [==============================] - 40s 8ms/step - loss: 0.7826 - acc: 0.8040 - val_loss: 1.9892 - val_acc: 0.6023\n",
      "Epoch 30/30\n",
      "5137/5137 [==============================] - 45s 9ms/step - loss: 0.7757 - acc: 0.8047 - val_loss: 1.9157 - val_acc: 0.6039\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f20a369f9b0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model \n",
    "model= Sequential()\n",
    "model.add(Embedding(len(word_index)+1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True))\n",
    "model.add(GRU(NB_LSTM))\n",
    "model.add(Dropout(PERC_DROPOUT))\n",
    "model.add((Dense(NB_CATEGORIES)))\n",
    "model.add(Activation('softmax')) # reminder sigmoid if is for binary classification\n",
    "model.compile(loss=sparse_categorical_crossentropy, optimizer=optz, metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=2, verbose=1,  mode='auto')\n",
    "checkpoint_filepath = os.path.join(logpath,  \"model.h1\")\n",
    "checkpoint_cb = ModelCheckpoint(checkpoint_filepath, save_best_only=True)\n",
    "model.fit(X_train, y_train.values,\n",
    "            validation_data=(X_test, y_test.values), \n",
    "            epochs=EPOCHS,\n",
    "            shuffle=True,\n",
    "            batch_size=64,\n",
    "            verbose=1,\n",
    "            callbacks=[tbcb , History()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = KeyedVectors.load_word2vec_format('../GoogleNews-vectors-negative300.bin', binary=True)\n",
    "vocabulary_size=min(len(word_index)+1,NUM_WORDS)\n",
    "embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i>=NUM_WORDS:\n",
    "        continue\n",
    "    try:\n",
    "        embedding_vector = word_vectors[word]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    except KeyError:\n",
    "        embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)\n",
    "\n",
    "del(word_vectors)\n",
    "\n",
    "from keras.layers import Embedding\n",
    "embedding_layer = Embedding(vocabulary_size,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.layers import Conv3D, MaxPooling3D\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, Conv2D, MaxPooling2D, Dropout,concatenate\n",
    "from keras.layers.core import Reshape, Flatten\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_51 (InputLayer)           (None, 382)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_53 (Embedding)        (None, 382, 50)      237300      input_51[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_34 (Dropout)            (None, 382, 50)      0           embedding_53[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 380, 10)      1510        dropout_34[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 375, 10)      4010        dropout_34[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 190, 10)      0           conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 187, 10)      0           conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_40 (Flatten)            (None, 1900)         0           max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_41 (Flatten)            (None, 1870)         0           max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_22 (Concatenate)    (None, 3770)         0           flatten_40[0][0]                 \n",
      "                                                                 flatten_41[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_35 (Dropout)            (None, 3770)         0           concatenate_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_36 (Dense)                (None, 51)           192321      dropout_35[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_37 (Dense)                (None, 51)           2652        dense_36[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 437,793\n",
      "Trainable params: 437,793\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 50          \n",
    "filter_sizes = (3, 8)\n",
    "num_filters = 10\n",
    "dropout_prob = (0.5, 0.8)\n",
    "\n",
    "inputs = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "embedding_layer = Embedding(len(word_index),\n",
    "                            embedding_dim,\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)\n",
    "embedding = embedding_layer(inputs)\n",
    "z = Dropout(dropout_prob[0])(embedding)\n",
    "# Convolutional block\n",
    "conv_blocks = []\n",
    "for sz in filter_sizes:\n",
    "    conv = Conv1D(filters=num_filters,\n",
    "                         kernel_size=sz,\n",
    "                         padding=\"valid\",\n",
    "                         activation=\"relu\",\n",
    "                         strides=1)(z)\n",
    "    conv = MaxPooling1D(pool_size=2)(conv)\n",
    "    conv = Flatten()(conv)\n",
    "    conv_blocks.append(conv)\n",
    "z = Concatenate()(conv_blocks) if len(conv_blocks) > 1 else conv_blocks[0]\n",
    "\n",
    "z = Dropout(dropout_prob[1])(z)\n",
    "z = Dense(51, activation=\"relu\")(z)\n",
    "output = Dense(51, activation=\"sigmoid\")(z)\n",
    "# this creates a model that includes\n",
    "model_cnn = Model(inputs, output)\n",
    "print(model_cnn.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = Adam(lr=1e-3)\n",
    "\n",
    "model_cnn.compile(loss=sparse_categorical_crossentropy,\n",
    "              optimizer=adam,\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5137 samples, validate on 1285 samples\n",
      "Epoch 1/10\n",
      "5137/5137 [==============================] - 11s 2ms/step - loss: 2.7442 - acc: 0.2548 - val_loss: 2.7836 - val_acc: 0.2498\n",
      "Epoch 2/10\n",
      "5137/5137 [==============================] - 11s 2ms/step - loss: 2.7286 - acc: 0.2540 - val_loss: 2.7731 - val_acc: 0.2514\n",
      "Epoch 3/10\n",
      "5137/5137 [==============================] - 11s 2ms/step - loss: 2.7181 - acc: 0.2628 - val_loss: 2.7672 - val_acc: 0.2599\n",
      "Epoch 4/10\n",
      "5137/5137 [==============================] - 13s 2ms/step - loss: 2.6979 - acc: 0.2626 - val_loss: 2.7551 - val_acc: 0.2545\n",
      "Epoch 5/10\n",
      "5137/5137 [==============================] - 13s 3ms/step - loss: 2.6925 - acc: 0.2622 - val_loss: 2.7459 - val_acc: 0.2700\n",
      "Epoch 6/10\n",
      "5137/5137 [==============================] - 12s 2ms/step - loss: 2.6861 - acc: 0.2694 - val_loss: 2.7363 - val_acc: 0.2864\n",
      "Epoch 7/10\n",
      "5137/5137 [==============================] - 14s 3ms/step - loss: 2.6885 - acc: 0.2714 - val_loss: 2.7248 - val_acc: 0.2918\n",
      "Epoch 8/10\n",
      "5137/5137 [==============================] - 14s 3ms/step - loss: 2.6763 - acc: 0.2758 - val_loss: 2.7317 - val_acc: 0.2988\n",
      "Epoch 9/10\n",
      "5137/5137 [==============================] - 17s 3ms/step - loss: 2.6635 - acc: 0.2838 - val_loss: 2.7093 - val_acc: 0.3004\n",
      "Epoch 10/10\n",
      "5137/5137 [==============================] - 17s 3ms/step - loss: 2.6475 - acc: 0.2790 - val_loss: 2.7004 - val_acc: 0.2981\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f205788a908>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cnn.fit(X_train, y_train.values,\n",
    "            validation_data=(X_test, y_test.values), \n",
    "            epochs=10,\n",
    "            shuffle=True,\n",
    "            batch_size=1000,\n",
    "            verbose=1,\n",
    "            callbacks=[tbcb , History()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
