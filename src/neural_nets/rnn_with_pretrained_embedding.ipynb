{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import os \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.losses import sparse_categorical_crossentropy, categorical_hinge\n",
    "from keras import optimizers\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "X = pd.read_csv('DATA/clean_data/cleaned_n_stemming_input_train.csv', sep=\";\", index_col=0)\n",
    "y = pd.read_csv('DATA/output_train.csv', sep=\";\", index_col=0)\n",
    "\n",
    "features = X.columns\n",
    "targets = y['intention'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = X['question'].iloc([0,80]).obj.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding taking into account  2714 words\n"
     ]
    }
   ],
   "source": [
    "word_model =gensim.models.KeyedVectors.load_word2vec_format(fname = 'DATA/trained_vectors/custom_word2vec/scratch_fr_stemming.bin', \n",
    "                                                            fvocab='DATA/trained_vectors/custom_word2vec/scratch_fr_vocab_stemming.txt', \n",
    "                                                            binary=True)\n",
    "print('embedding taking into account ', len( word_model.vocab), 'words') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2idx(word):\n",
    "    if word in word_model.vocab.keys():\n",
    "        return word_model.vocab[word].index\n",
    "    else:\n",
    "        return 0\n",
    "def idx2word(idx):\n",
    "    return word_model.index2word[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "382"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find max size of the sequences \n",
    "MAX_SEQUENCE_LENGTH = 0 \n",
    "for sentence in X['question']:\n",
    "    if MAX_SEQUENCE_LENGTH<len(sentence.split()):\n",
    "        MAX_SEQUENCE_LENGTH = len(sentence.split())\n",
    "MAX_SEQUENCE_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.028489382519683783"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_model.similarity('malad', 'jeud')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47308977662157237"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_model.similarity('vendred', 'jeud')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19189552314385092"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_model.similarity('malad', 'enceint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8028, 382)\n"
     ]
    }
   ],
   "source": [
    "X_embedded = np.zeros([len(sentences), MAX_SEQUENCE_LENGTH], dtype=np.int32)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, word in enumerate(sentence.split()):\n",
    "        X_embedded[i, t] = word2idx(word)\n",
    "print(X_embedded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test,y_train, y_test = train_test_split(X_embedded,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try RNN with pretrained embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_unique_logpath(logdir, raw_run_name):\n",
    "        i = 0\n",
    "        while(True):\n",
    "                run_name = raw_run_name + \"-\" + str(i)\n",
    "                log_path = os.path.join(logdir, run_name)\n",
    "                if not os.path.isdir(log_path):\n",
    "                        return log_path\n",
    "                i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = \"gru_\"+str(NB_LSTM)+\"_rmsprop_\"+str(LEARNING_RATE)\n",
    "run_name += \"_decay_embedding_\"+str(EMBEDDING_DIM)\n",
    "run_name +=\"_dropout_\"+str(PERC_DROPOUT)+\"_early_stop_shuffle\"\n",
    "\n",
    "run_name += \"sanity_check\"\n",
    "\n",
    "logpath = generate_unique_logpath(\"./logs_tensorboard\", run_name)\n",
    "tbcb = TensorBoard(log_dir=logpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, None, 100)         271400    \n",
      "_________________________________________________________________\n",
      "gru_6 (GRU)                  (None, 50)                22650     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 51)                2601      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 51)                0         \n",
      "=================================================================\n",
      "Total params: 296,651\n",
      "Trainable params: 25,251\n",
      "Non-trainable params: 271,400\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 6021 samples, validate on 2007 samples\n",
      "Epoch 1/10\n",
      "6021/6021 [==============================] - 30s 5ms/step - loss: nan - acc: 0.0274 - val_loss: nan - val_acc: 0.0254\n",
      "Epoch 2/10\n",
      "6021/6021 [==============================] - 32s 5ms/step - loss: nan - acc: 0.0254 - val_loss: nan - val_acc: 0.0254\n",
      "Epoch 00002: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa445aecf98>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Define parameters \n",
    "#EMBEDDING_DIM = 200\n",
    "NB_CATEGORIES = len(targets)\n",
    "NB_LSTM = 50\n",
    "PERC_DROPOUT = 0.5\n",
    "EPOCHS = 10\n",
    "#Define RMSProp optimizer\n",
    "LEARNING_RATE = 0.06\n",
    "RATE_DECAY = LEARNING_RATE / EPOCHS\n",
    "\n",
    "optz = optimizers.RMSprop(lr=LEARNING_RATE, decay=RATE_DECAY)\n",
    "sgd = optimizers.SGD(lr=LEARNING_RATE, decay=RATE_DECAY, momentum=0.9, nesterov=True)\n",
    "\n",
    "\n",
    "#Model \n",
    "model= Sequential()\n",
    "model.add(word_model.get_keras_embedding())\n",
    "'''\n",
    "model.add(Embedding(len(word_index)+1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            weights = \n",
    "                            trainable=True))\n",
    "'''\n",
    "model.add(GRU(NB_LSTM))\n",
    "model.add(Dropout(PERC_DROPOUT))\n",
    "model.add((Dense(NB_CATEGORIES)))\n",
    "model.add(Activation('softmax')) # reminder sigmoid if is for binary classification\n",
    "model.compile(loss=sparse_categorical_crossentropy, optimizer=optz, metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=2, verbose=1,  mode='auto')\n",
    "checkpoint_filepath = os.path.join(logpath,  \"model.h1\")\n",
    "checkpoint_cb = ModelCheckpoint(checkpoint_filepath, save_best_only=True)\n",
    "model.fit(X_train, y_train.values,\n",
    "            validation_data=(X_test, y_test.values), \n",
    "            epochs=EPOCHS,\n",
    "            shuffle=True,\n",
    "            batch_size=64,\n",
    "            verbose=1,\n",
    "            callbacks=[tbcb, early_stop])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
