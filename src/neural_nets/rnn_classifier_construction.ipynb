{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from utils import train_test_validation_split\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Embedding, Flatten, LSTM, GRU\n",
    "from keras.losses import sparse_categorical_crossentropy, categorical_hinge\n",
    "from keras import optimizers\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "X = pd.read_csv('DATA/clean_data/cleaning_n_stemming_input_train.csv', sep=\";\", index_col=0)\n",
    "y = pd.read_csv('DATA/output_train.csv', sep=\";\", index_col=0)\n",
    "\n",
    "features = X.columns\n",
    "targets = y['intention'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    bonjour tromp forum question alor repos ici pr...\n",
       "1                  <MEDICAMENT> soulag contr <MALADIE>\n",
       "2    medecin <MEDICAMENT> prescr <MEDICAMENT> <ORDI...\n",
       "3       est exist form adapt enfant <AGE> <MEDICAMENT>\n",
       "4    medecin soign <MEDICAMENT> pharyngit <MEDICAME...\n",
       "Name: question, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X['question'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for nn and find correct parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimation of the size of the vocabulary \n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit_transform(X['question'])\n",
    "MAX_NB_WORDS = len(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "382"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find max size of the sequences \n",
    "MAX_SEQUENCE_LENGTH = 0 \n",
    "for sentence in X['question']:\n",
    "    if MAX_SEQUENCE_LENGTH<len(sentence.split()):\n",
    "        MAX_SEQUENCE_LENGTH = len(sentence.split())\n",
    "MAX_SEQUENCE_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4746 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# Preprocess text fo feed the net \n",
    "texts = X['question']\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "X_sequences = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, X_validation, y_train, y_test, y_validation = train_test_validation_split(X_sequences,y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train / (len(word_index)-1)\n",
    "X_test = X_test / (len(word_index)-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_unique_logpath(logdir, raw_run_name):\n",
    "        i = 0\n",
    "        while(True):\n",
    "                run_name = raw_run_name + \"-\" + str(i)\n",
    "                log_path = os.path.join(logdir, run_name)\n",
    "                if not os.path.isdir(log_path):\n",
    "                        return log_path\n",
    "                i = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define parameters \n",
    "EMBEDDING_DIM = 200\n",
    "NB_CATEGORIES = len(targets)\n",
    "NB_LSTM = 50\n",
    "PERC_DROPOUT = 0.2 # to test between 0.2 and 0.5\n",
    "EPOCHS = 10000\n",
    "#Define RMSProp optimizer\n",
    "LEARNING_RATE = 0.006\n",
    "RATE_DECAY = 6e-4\n",
    "optz = optimizers.RMSprop(lr=LEARNING_RATE, decay=RATE_DECAY)\n",
    "\n",
    "\n",
    "sgd = optimizers.SGD(lr=LEARNING_RATE, decay=RATE_DECAY, momentum=0.9, nesterov=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = \"gru_norm_\"+str(NB_LSTM)+\"_rmsprop_\"+str(LEARNING_RATE)\n",
    "run_name += \"_decay_embedding_\"+str(EMBEDDING_DIM)\n",
    "run_name +=\"_dropout_\"+str(PERC_DROPOUT)+\"_early_stop_shuffle\"\n",
    "\n",
    "logpath = generate_unique_logpath(\"./logs_tensorboard\", run_name)\n",
    "tbcb = TensorBoard(log_dir=logpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:1238: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:1340: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:1204: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 382, 200)          949400    \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 50)                37650     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 51)                2601      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 51)                0         \n",
      "=================================================================\n",
      "Total params: 989,651\n",
      "Trainable params: 989,651\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 5137 samples, validate on 1285 samples\n",
      "Epoch 1/10000\n",
      "5137/5137 [==============================] - 45s 9ms/step - loss: 3.2698 - acc: 0.2110 - val_loss: 3.2686 - val_acc: 0.2171\n",
      "Epoch 2/10000\n",
      "5137/5137 [==============================] - 50s 10ms/step - loss: 3.2156 - acc: 0.2211 - val_loss: 3.2651 - val_acc: 0.2171\n",
      "Epoch 3/10000\n",
      "5137/5137 [==============================] - 44s 9ms/step - loss: 3.2079 - acc: 0.2209 - val_loss: 3.2477 - val_acc: 0.2171\n",
      "Epoch 4/10000\n",
      "5137/5137 [==============================] - 48s 9ms/step - loss: 3.2017 - acc: 0.2209 - val_loss: 3.2540 - val_acc: 0.2171\n",
      "Epoch 5/10000\n",
      "5137/5137 [==============================] - 55s 11ms/step - loss: 3.1936 - acc: 0.2209 - val_loss: 3.2376 - val_acc: 0.2171\n",
      "Epoch 6/10000\n",
      "5137/5137 [==============================] - 46s 9ms/step - loss: 3.1922 - acc: 0.2209 - val_loss: 3.2508 - val_acc: 0.2171\n",
      "Epoch 7/10000\n",
      "5137/5137 [==============================] - 49s 10ms/step - loss: 3.1931 - acc: 0.2209 - val_loss: 3.2400 - val_acc: 0.2171\n",
      "Epoch 8/10000\n",
      "4096/5137 [======================>.......] - ETA: 9s - loss: 3.2053 - acc: 0.2161"
     ]
    }
   ],
   "source": [
    "#Model \n",
    "model= Sequential()\n",
    "model.add(Embedding(len(word_index)+1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True))\n",
    "model.add(GRU(NB_LSTM))\n",
    "model.add(Dropout(PERC_DROPOUT))\n",
    "model.add((Dense(NB_CATEGORIES)))\n",
    "model.add(Activation('softmax')) # reminder sigmoid if is for binary classification\n",
    "model.compile(loss=sparse_categorical_crossentropy, optimizer=optz, metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=2, verbose=1,  mode='auto')\n",
    "checkpoint_filepath = os.path.join(logpath,  \"model.h1\")\n",
    "checkpoint_cb = ModelCheckpoint(checkpoint_filepath, save_best_only=True)\n",
    "model.fit(X_train, y_train,\n",
    "            validation_data=(X_test, y_test), \n",
    "            epochs=EPOCHS,\n",
    "            shuffle=True,\n",
    "            batch_size=64,\n",
    "            verbose=1,\n",
    "            callbacks=[tbcb])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = \"no_stemming_gru_\"+str(NB_LSTM)+\"_rmsprop_\"+str(LEARNING_RATE)\n",
    "run_name += \"_decay_embedding_\"+str(EMBEDDING_DIM)\n",
    "run_name +=\"_dropout_\"+str(PERC_DROPOUT)+\"_early_stop_shuffle\"\n",
    "\n",
    "logpath = generate_unique_logpath(\"./logs_tensorboard\", run_name)\n",
    "tbcb = TensorBoard(log_dir=logpath)\n",
    "\n",
    "# TEST 2 \n",
    "PERC_DROPOUT = 0.2 # to test between 0.2 and 0.5\n",
    "\n",
    "#Model \n",
    "model2= Sequential()\n",
    "model2.add(Embedding(len(word_index)+1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True))\n",
    "model2.add(GRU(NB_LSTM))\n",
    "model2.add(Dropout(PERC_DROPOUT))\n",
    "model2.add((Dense(NB_CATEGORIES)))\n",
    "model2.add(Activation('softmax')) # reminder sigmoid if is for binary classification\n",
    "model2.compile(loss=sparse_categorical_crossentropy, optimizer=optz, metrics=['accuracy'])\n",
    "print(model2.summary())\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=2, verbose=1,  mode='auto')\n",
    "checkpoint_filepath = os.path.join(logpath,  \"model.h1\")\n",
    "checkpoint_cb = ModelCheckpoint(checkpoint_filepath, save_best_only=True)\n",
    "model2.fit(X_train, y_train,\n",
    "            validation_data=(X_test, y_test), \n",
    "            epochs=EPOCHS,\n",
    "            shuffle=True,\n",
    "            batch_size=64,\n",
    "            verbose=1,\n",
    "            callbacks=[tbcb])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
