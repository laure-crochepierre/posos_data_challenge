{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "import enchant\n",
    "import re\n",
    "import sys\n",
    "import ipdb\n",
    "import unidecode\n",
    "\n",
    "from replacers import SpellingReplacer\n",
    "from nltk.stem.snowball import FrenchStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data source path \n",
    "data_source_path = \"../../DATA/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(data_source_path+'input_train.csv', sep=\";\")\n",
    "y = pd.read_csv(data_source_path+'output_train.csv', sep=\";\")\n",
    "\n",
    "features = X.columns\n",
    "targets = y['intention'].unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['﻿ID', 'question'], dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>﻿ID</th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>bonjour,  je m suis trompé de forum pour ma qu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>est ce que le motilium me soulagera contre les...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>mon médecin m'a prescrit adenyl. au 2ème cache...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Est-ce qu'il existe une forme adaptée aux enfa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>mon  medecin  me soigne  pour  une  rhino  pha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ﻿ID                                           question\n",
       "0    0  bonjour,  je m suis trompé de forum pour ma qu...\n",
       "1    1  est ce que le motilium me soulagera contre les...\n",
       "2    2  mon médecin m'a prescrit adenyl. au 2ème cache...\n",
       "3    3  Est-ce qu'il existe une forme adaptée aux enfa...\n",
       "4    4  mon  medecin  me soigne  pour  une  rhino  pha..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the list of medicaments\n",
    "medicaments = [x[:-1] for x in open(data_source_path+\"../DATA/medicaments.txt\", \"r\").readlines()]\n",
    "symptomes = [x[:-1] for x in open(data_source_path+\"symptomes.txt\", \"r\").readlines()]\n",
    "maladies = [x[:-1] for x in open(data_source_path+\"maladies.txt\", \"r\").readlines()]\n",
    "posologies = [\"mg\", \"ml\", \"ch\", \"g\", \"l\"]\n",
    "replacer = SpellingReplacer(dict_name='fr_FR')\n",
    "french_stemmer = FrenchStemmer()\n",
    "lemmatizer = spacy.load('fr')\n",
    "\n",
    "stop_words = [ 'ce', 'ces', 'de', 'des', 'du', 'elle', 'eux', 'il', 'je', 'la', 'le', 'leur', 'lui', 'ma', 'mais', 'me', 'mes', 'moi', 'mon', 'nos', 'notre', 'nous', 'on', 'sa', 'se', 'ses', 'son', 'ta', 'te', 'tes', 'toi', 'ton', 'tu', 'un', 'une', 'vos', 'votre', 'vous', 'c', 'd', 'j', 'l', 'm', 'n', 's', 't', 'y']\n",
    "etre = ['été','étée','étées', 'étés', 'étant', 'étante','étants', 'étantes','suis', 'es','est', 'sommes','êtes','sont', 'serai', 'seras', 'sera', 'serons',  'serez', 'seront', 'serais', 'serait', 'serions', 'seriez', 'seraient', 'étais', 'était', 'étions', 'étiez', 'étaient', 'fus', 'fut', 'fûmes', 'fûtes', 'furent', 'sois', 'soit', 'soyons', 'soyez', 'soient', 'fusse', 'fusses', 'fût', 'fussions', 'fussiez', 'fussent']\n",
    "avoir = ['ayant', 'ayante', 'ayantes', 'ayants', 'eu', 'eue', 'eues', 'eus', 'ai', 'as', 'avons', 'avez', 'ont', 'aurai', 'auras', 'aura', 'aurons', 'aurez', 'auront', 'aurais', 'aurait', 'aurions', 'auriez', 'auraient', 'avais', 'avait', 'avions',  'aviez', 'avaient', 'eut', 'eûmes', 'eûtes', 'eurent', 'aie', 'aies', 'ait', 'ayons', 'ayez', 'aient', 'eusse', 'eusses', 'eût', 'eussions', 'eussiez', 'eussent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indicateur d'avancement \n",
    "total = len(X['question'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List of all the preprocessing done "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost in order :\n",
    "- remove **ponctuation**\n",
    "- remove all **stop words**\n",
    "- test if word is in list of **drugs**\n",
    "- test if word is a **posology**\n",
    "- test if word is a **disease**\n",
    "- test if word is a **symptom**\n",
    "- test if word is an **hour**\n",
    "- test if word is an **age**\n",
    "- test if word is **ordinal number**\n",
    "- replace a maximum of **typo mistakes** with pyenchant : \n",
    "    from a list of suggestions, take the most probable one, \n",
    "    test if it is a stop word \n",
    "    replace the word only if the most probable suggestion is more than 1 letter \n",
    "- **lemmatizing** of words \n",
    "- remove **accents**\n",
    "- remove **short words** of less than 2 letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(word):\n",
    "    word = word.lower()\n",
    "    heure_re = r'[0-9]{1,2}h[0-9]{,2}$'\n",
    "    ordinal_re = r'[0-9]+(er|ème|ère|eme|ere)$'\n",
    "    age_re = r'[0-9]+(ans|an)$'\n",
    "    posologie_re = r'[0-9]+(mg|g|l|ml|ch|milligramme|milligrammes|milligramm)$'\n",
    "    if word in etre: \n",
    "        word = 'être'\n",
    "    elif word in avoir:\n",
    "        word = 'avoir'\n",
    "    elif word == '+':\n",
    "        word = 'plus'\n",
    "    elif word == '=':\n",
    "        word = 'égal'    \n",
    "    elif word in medicaments:\n",
    "        word = \"médicament\" #\"<MEDICAMENT>\" \n",
    "        count['medicaments']+=1\n",
    "    elif word in posologies: \n",
    "        word = \"composition\" #\"<POSOLOGIE>\"\n",
    "        count['doses']+=1\n",
    "    elif re.match(posologie_re,word):\n",
    "        word = \"composition\" #\"<COMPOSITION>\" \n",
    "        count['doses']+=1\n",
    "    elif word in maladies:\n",
    "        word = \"maladie\" #\"<MALADIE>\"\n",
    "        count['maladies']+=1\n",
    "    elif word in symptomes:\n",
    "        word = \"symptôme\" #\"<SYMPTOME>\" \n",
    "        count['symptomes']+=1\n",
    "    elif re.match(heure_re,word):\n",
    "        word = \"heure\" #\"<HEURE>\" \n",
    "        count['heures']+=1\n",
    "    elif word == 'DCI':\n",
    "        word = \"dénomination commune internationale\"\n",
    "    elif re.match(age_re,word):\n",
    "        word = \"âge\" #\"<AGE>\" \n",
    "        count['ages']+=1\n",
    "    elif re.match(ordinal_re,word):\n",
    "        word = \"ordinal\" #\"<ORDINAL>\"\n",
    "        count['ordinal']+=1\n",
    "    else:\n",
    "        suggestion = replacer.replace(word)\n",
    "        if suggestion != word: \n",
    "            count['corrections']+=1\n",
    "            valid_suggestion = re.sub(r'[^\\w]', ' ', suggestion).split()\n",
    "            if valid_suggestion[len(valid_suggestion)-1] not in stop_words:\n",
    "                word = valid_suggestion[len(valid_suggestion)-1].lower()\n",
    "    word = french_stemmer.stem(word)\n",
    "    #return unidecode.unidecode(word)\n",
    "    return word\n",
    "    \n",
    "def preprocess(row):\n",
    "    sentence = ''.join([re.sub(r'[^\\w\\+=]', ' ', word) for word in row['question']]) \n",
    "    word_list = [clean(word) for word in sentence.split() if (word not in stop_words)]\n",
    "    #lemma_list = [unidecode.unidecode(word.lemma_) for word in lemmatizer(' '.join(word for word in word_list))]\n",
    "    preprocessed_sentence = ' '.join(word for word in word_list if(not re.match( r'^[0-9]*$',word)) )\n",
    "    \n",
    "    if row['﻿ID']%100 == 0:\n",
    "        print(\" row {} / {}\".format(row['﻿ID'], total),end='\\r')\n",
    "    return preprocessed_sentence\n",
    "\n",
    "def output_format(row):\n",
    "    return row[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'heures': 168, 'doses': 648, 'corrections': 6686, 'symptomes': 311, 'ages': 40, 'ordinal': 180, 'maladies': 1830, 'medicaments': 9937}\n"
     ]
    }
   ],
   "source": [
    "count = {'medicaments' : 0,'doses': 0,'maladies': 0,'symptomes' :0, \"corrections\": 0, \"ages\":0, \"heures\":0, \"ordinal\":0}\n",
    "X_sub = X.iloc[:10]\n",
    "X_clean = X.apply(preprocess, axis = 1)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'médecin médic prescr médic au ordinal cachet malad têt terribl et au ordinal malad symptôm froid chaleur intens dan têt symptôm fourmill dan levr supérieur difficult à respir des arrêt médic tous le symptôm avoir disparu cel être déjà arriv à quelqu'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_clean[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do the same for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'heures': 39, 'doses': 172, 'corrections': 1589, 'symptomes': 70, 'ages': 10, 'ordinal': 33, 'maladies': 450, 'medicaments': 2515}\n"
     ]
    }
   ],
   "source": [
    "X_test = pd.read_csv(data_source_path+'input_test.csv', sep=\";\")\n",
    "total = len(X_test['question']) + len(X['question'])\n",
    "count = {'medicaments' : 0,'doses': 0,'maladies': 0,'symptomes' :0, \"corrections\": 0, \"ages\":0, \"heures\":0, \"ordinal\":0}\n",
    "X_test_clean = X_test.apply(preprocess, axis = 1)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat = \"stemming\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_clean.to_csv(data_source_path+'input_train/'+strat+'/clean_input_to_train.csv', sep=';', header = ['question'])\n",
    "X_test_clean.to_csv(data_source_path+'input_test/'+strat+'/clean_input_to_test.csv', sep=';', header = ['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
